import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;




public class MaxAmt {
	
	public static class MapperEx extends Mapper<LongWritable,Text,Text,LongWritable>
	 {
		public void map(LongWritable k,Text v, Context con) throws IOException, InterruptedException{
		
			String Line =v.toString();
			String[] arr =Line.split(";");
			String Cust_id= arr[1];
			long amt = Long.parseLong(arr[8]);
			con.write(new Text(Cust_id),new LongWritable(amt));
			
		}
	}
	
	public static class ReducerEx extends Reducer<Text,LongWritable,Text,LongWritable>{
		public void reduce(Text key,Iterable<LongWritable> values, Context con) throws IOException, InterruptedException{
			long sum=0;
			for(LongWritable val:values)
			sum=sum+val.get();
			
	        con.write(key,new LongWritable(sum));
			
		}
	}

	
	public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
		
		 Configuration conf = new Configuration();
		 Job job = Job.getInstance(conf, "Volume Count");
		    job.setJarByClass(MaxAmt.class);
		    job.setMapperClass(MapperEx.class);
		    job.setReducerClass(ReducerEx.class);
		    
		    job.setOutputKeyClass(Text.class);
		    job.setOutputValueClass(LongWritable.class);
		    FileInputFormat.addInputPath(job, new Path(args[0]));
		    FileOutputFormat.setOutputPath(job, new Path(args[1]));
		    System.exit(job.waitForCompletion(true) ? 0 : 1);

	}

}


/*import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;



public class MaxAmt{
	
	public static class MapperEx extends Mapper<Object, Text, Text, Text> {

		public void map(Object key,Text value,Context ctx) throws IOException, InterruptedException
		{
		String[] arr=value.toString().split(";");
		String Cust_Id = arr[1];
		String Cost = arr[7];
		//int sal = Integer.parseInt(arr[4]);
		ctx.write(new Text(Cost), new Text(Cust_Id));
		}
		}
	
	public static class ReducerEx extends Reducer<Text, Text,Text, Text>
	{
	public void reduce(Text key,Iterable<Text> itr,Context context) throws IOException, InterruptedException
	{ 
	int maxamt=0;
	String cust= "";
	String amt ="";

	for (Text val : itr){
	String arr[] = val.toString().split(";");
	if (maxamt < Integer.parseInt(arr[1]))
	{
	maxamt = Integer.parseInt(arr[1]);
	amt = arr[1].toString();

	cust = arr[0].toString();

	}

	}
	context.write(new Text(key), new Text(cust.toString() +"-" + amt.toString()));

	}

	}
	
	
public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException{
		
	
	Configuration conf=new Configuration();
	Job job=Job.getInstance(conf);
	job.setJarByClass(MaxAmt.class);
	job.setMapperClass(MapperEx.class);
	job.setReducerClass(ReducerEx.class);
	
	job.setOutputKeyClass(Text.class);
	job.setOutputValueClass(Text.class);
	
	FileInputFormat.addInputPath(job, new Path(args[0]));
	FileOutputFormat.setOutputPath(job, new Path(args[1]));
	System.exit(job.waitForCompletion(true) ? 0 : 1);
	
}
}*/