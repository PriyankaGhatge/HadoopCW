import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Reducer.Context;


public class Question1a {
	
	public static class MapperEx extends Mapper<LongWritable,Text,Text,IntWritable>
		{
		
		public void map(LongWritable k,Text v, Context con) throws IOException, InterruptedException{
			
			String Line =v.toString();
			String[] arr =Line.split("\t");
			
			String SOC= arr[3];
			String Job_Title= arr[4];
			int count=0;
			if(Job_Title.contains("DATA SCIENTIST")){
				count++;
			
			con.write(new Text(SOC),new IntWritable(count));
			}
		}
			
		}

	public static class ReducerEx extends Reducer<Text,IntWritable,Text,IntWritable>{
		int max_job =0;
		String Industry="";
		
		public void reduce(Text key,Iterable<IntWritable> values, Context con) {
			
			int count=0;

				for(IntWritable val:values)
				{
					count+=val.get();

					if (max_job < count){
						max_job=count;
						
						Industry=key.toString();
					}
				
			}
			}

			public void cleanup(Context con) throws IOException, InterruptedException{
				
				con.write(new Text(Industry),new IntWritable(max_job));
			}

		}
			
		

	
	public static void main(String[] args) {
		

	}

}
